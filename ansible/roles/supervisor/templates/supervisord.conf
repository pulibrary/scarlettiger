; supervisor config file

[supervisord]
logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)
pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)
childlogdir=/var/log/supervisor            ; ('AUTO' child log dir, default $TEMP)

; the below section must remain in the config file for RPC
; (supervisorctl/web interface) to work, additional interfaces may be
; added by defining them in separate rpcinterface: sections

[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

[inet_http_server]          ; inet (TCP) server disabled by default
port=127.0.0.1:9001         ; (ip_address:port specifier, *:port for all iface)
#username=admin      ; (default is no username (open server))
#password=pass ; (default is no password (open server))

[supervisorctl]
serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket
#username=admin       ; should be same as http_username if set
#password=pass  ; should be same as http_password if set

[program:namenode]
command = /opt/hadoop/bin/hdfs --config /opt/hadoop/etc/hadoop namenode
stdout_logfile = /var/log/hadoop/namenode.stdout
stderr_logfile = /var/log/hadoop/namenode.stderr
autostart = true
user = combine

[program:datanode]
command = /opt/hadoop/bin/hdfs --config /opt/hadoop/etc/hadoop datanode
stdout_logfile = /var/log/hadoop/datanode.stdout
stderr_logfile = /var/log/hadoop/datanode.stderr
autostart = true
user = combine

[group:hdfs]
programs=namenode,datanode

[program:spark_driver]
environment =
    SPARK_MASTER_HOST=127.0.0.1
command=/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
directory=/opt/spark/
autostart = false
autorestart = true
stdout_logfile = /var/log/spark/spark_driver.stdout
stderr_logfile = /var/log/spark/spark_driver.stderr
user = combine

[program:spark_worker]
command=/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077
directory=/opt/spark
autostart = false
autorestart = true
stdout_logfile = /var/log/spark/spark_worker.stdout
stderr_logfile = /var/log/spark/spark_worker.stderr
user = combine

[group:spark]
programs=spark_driver,spark_worker

[program:livy]
command = /opt/livy/bin/livy-server
stdout_logfile = /var/log/livy/livy.stdout
stderr_logfile = /var/log/livy/livy.stderr
autostart = true
user = combine

[program:pyjxslt]
command = /usr/local/anaconda/envs/combine/bin/pyjxslt 6767
stdout_logfile = /var/log/pyjxslt/pyjxslt.stdout
stderr_logfile = /var/log/pyjxslt/pyjxslt.stderr
autostart = true
user = combine

[program:gunicorn]
directory=/opt/combine
command=/usr/local/anaconda/envs/combine/bin/gunicorn --workers 3 --bind unix:/opt/combine/combine.sock combine.wsgi:application
autostart=true
autorestart=true
stderr_logfile=/var/log/gunicorn/gunicorn.stdout
stdout_logfile=/var/log/gunicorn/gunicorn.stderr
user=combine
group=www-data
environment=LANG=en_US.UTF-8,LC_ALL=en_US.UTF-8

[program:celery]
directory=/opt/combine
command=/usr/local/anaconda/envs/combine/bin/celery -A core worker -l info --concurrency 1
autostart=true
autorestart=true
stderr_logfile=/var/log/celery.stdout
stdout_logfile=/var/log/celery.stderr
user=combine
